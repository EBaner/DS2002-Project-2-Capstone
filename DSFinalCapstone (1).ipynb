{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5d216b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in c:\\users\\baner\\anacondaa\\envs\\pysparkenv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: pymongo in c:\\users\\baner\\anacondaa\\envs\\pysparkenv\\lib\\site-packages (4.12.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\baner\\anacondaa\\envs\\pysparkenv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\baner\\anacondaa\\envs\\pysparkenv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\baner\\anacondaa\\envs\\pysparkenv\\lib\\site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\baner\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\baner\\anacondaa\\envs\\pysparkenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\baner\\anacondaa\\envs\\pysparkenv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\baner\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "C:\\spark-3.5.4-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## Overview\n",
    "# This notebook implements a dimensional Data Lakehouse based on the AdventureWorks dataset to analyze sales transactions.\n",
    "# \n",
    "# ## Architecture\n",
    "# - Data Sources: MySQL (AdventureWorks), CSV files, JSON streaming files, MongoDB Atlas\n",
    "# - Integration Pattern: ELT (Extract, Load, Transform)\n",
    "# - Lakehouse Architecture: Databricks Bronze, Silver, Gold layers\n",
    "# \n",
    "# ## Dimensional Model\n",
    "# - Fact Table: Sales (from SalesOrderDetail)\n",
    "# - Dimension Tables: \n",
    "# - Date\n",
    "# - Customer\n",
    "# - Product\n",
    "# - Employee (Sales Person)\n",
    "# - Territory\n",
    "\n",
    "# Install necessary packages\n",
    "%pip install pymysql pymongo pandas numpy\n",
    "\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format, year, month, dayofmonth, weekofyear, dayofweek, from_json, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from sqlalchemy import create_engine, text\n",
    "import certifi\n",
    "import json\n",
    "import os\n",
    "from sqlalchemy.exc import OperationalError\n",
    "import numpy as np\n",
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d324e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'adventureworks')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "lakehouse_dir = os.path.join(base_dir, 'adventureworks_lakehouse')\n",
    "os.makedirs(lakehouse_dir, exist_ok=True)\n",
    "\n",
    "bronze_dir = os.path.join(lakehouse_dir, 'bronze')\n",
    "silver_dir = os.path.join(lakehouse_dir, 'silver')\n",
    "gold_dir = os.path.join(lakehouse_dir, 'gold')\n",
    "\n",
    "for dir_path in [bronze_dir, silver_dir, gold_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2abb283",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mongodb_args = {\n",
    "    \"user_name\": \"banerjeeethan\",\n",
    "    \"password\": \"VUHXrKOGS58xzQyE\",\n",
    "    \"cluster_name\": \"Cluster0\",\n",
    "    \"cluster_subnet\": \"38mdy\",\n",
    "    \"cluster_location\": \"atlas\",  # \"local\"\n",
    "    \"db_name\": \"adventureworks\"\n",
    "}\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    if args[\"cluster_location\"] == \"atlas\":\n",
    "        connect_str = f\"mongodb+srv://{args['user_name']}:{args['password']}@{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net\"\n",
    "        client = pymongo.MongoClient(connect_str, tlsCAFile=certifi.where())\n",
    "    else:\n",
    "        client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    return client\n",
    "\n",
    "# Function to fetch MongoDB data\n",
    "def get_mongo_dataframe(mongo_client, db_name, collection, query={}):\n",
    "    db = mongo_client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    if '_id' in dframe and '_id' in dframe.columns:\n",
    "        dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f98dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_mysql(query):\n",
    "    connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='password',  \n",
    "        database='adventureworks'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_sql(query, connection)\n",
    "        return df\n",
    "    finally:\n",
    "        connection.close()\n",
    "        \n",
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e55f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Initialize Spark Session\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_spark_conf_args(spark_jars=None, **args):\n",
    "    \"\"\"Generate Spark configuration arguments.\"\"\"\n",
    "    import os\n",
    "    from multiprocessing import cpu_count as get_cpu_count\n",
    "    \n",
    "    # Ensure spark_jars is a list\n",
    "    spark_jars = spark_jars or []\n",
    "    jars = \", \".join(spark_jars) if spark_jars else \"\"\n",
    "    \n",
    "    cpu_count = get_cpu_count() or 2\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\": \"PySpark AdventureWorks Data Lakehouse\",\n",
    "        \"shuffle_partitions\": str(2 if cpu_count < 2 else cpu_count), \n",
    "        \"mongo_uri\": args.get(\n",
    "            \"mongo_uri\",\n",
    "            f\"mongodb+srv://{args['user_name']}:{args['password']}@{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/{args['db_name']}\"\n",
    "        ),\n",
    "        \"spark_jars\": jars,\n",
    "        \"database_dir\": os.path.abspath('spark-warehouse')\n",
    "    }\n",
    "    return sparkConf_args\n",
    "\n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name']) \\\n",
    "        .setMaster(args['worker_threads']) \\\n",
    "        .set('spark.driver.memory', '4g') \\\n",
    "        .set('spark.executor.memory', '4g') \\\n",
    "        .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "        .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "        .set('spark.sql.debug.maxToStringFields', '35') \\\n",
    "        .set('spark.sql.shuffle.partitions', str(args['shuffle_partitions'])) \\\n",
    "        .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "        .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "        .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    if args['spark_jars']:\n",
    "        sparkConf = sparkConf.set('spark.jars', args['spark_jars'])\n",
    "        \n",
    "    if 'mongo_uri' in args:\n",
    "        sparkConf = sparkConf \\\n",
    "            .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "            .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "            .set('spark.mongodb.output.uri', args['mongo_uri'])\n",
    "            \n",
    "    return sparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bca663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Upload Employee Data to MongoDB\n",
    "# ------------------------------------------------------------------------------\n",
    "def upload_employee_data_to_mongodb():\n",
    "    \"\"\"Extract employee data from MySQL and upload to MongoDB.\"\"\"\n",
    "    print(\"Extracting employee data from MySQL and uploading to MongoDB...\")\n",
    "    employee_query = \"\"\"\n",
    "    SELECT \n",
    "        e.EmployeeID,\n",
    "        e.NationalIDNumber,\n",
    "        e.LoginID,\n",
    "        e.ManagerID,\n",
    "        e.Title,\n",
    "        e.BirthDate,\n",
    "        e.MaritalStatus,\n",
    "        e.Gender,\n",
    "        e.HireDate,\n",
    "        e.SalariedFlag,\n",
    "        e.VacationHours,\n",
    "        e.SickLeaveHours,\n",
    "        edh.DepartmentID,\n",
    "        d.Name as DepartmentName,\n",
    "        edh.ShiftID,\n",
    "        s.Name as ShiftName,\n",
    "        edh.StartDate,\n",
    "        edh.EndDate\n",
    "    FROM employee e\n",
    "    JOIN employeedepartmenthistory edh ON e.EmployeeID = edh.EmployeeID\n",
    "    JOIN department d ON edh.DepartmentID = d.DepartmentID\n",
    "    JOIN shift s ON edh.ShiftID = s.ShiftID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        employee_df = extract_from_mysql(employee_query)\n",
    "        print(f\"Extracted {len(employee_df)} employee records with department and shift\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting employee data with department and shift: {str(e)}\")\n",
    "        employee_query = \"\"\"\n",
    "        SELECT \n",
    "            e.EmployeeID,\n",
    "            e.NationalIDNumber,\n",
    "            e.LoginID,\n",
    "            e.ManagerID,\n",
    "            e.Title,\n",
    "            e.BirthDate,\n",
    "            e.MaritalStatus,\n",
    "            e.Gender,\n",
    "            e.HireDate,\n",
    "            e.SalariedFlag,\n",
    "            e.VacationHours,\n",
    "            e.SickLeaveHours,\n",
    "            edh.DepartmentID,\n",
    "            edh.ShiftID,\n",
    "            edh.StartDate,\n",
    "            edh.EndDate\n",
    "        FROM employee e\n",
    "        JOIN employeedepartmenthistory edh ON e.EmployeeID = edh.EmployeeID\n",
    "        \"\"\"\n",
    "        employee_df = extract_from_mysql(employee_query)\n",
    "        print(f\"Extracted {len(employee_df)} employee records (without department/shift names)\")\n",
    "    \n",
    "    # Upload to MongoDB\n",
    "    \n",
    "    for col in employee_df.select_dtypes(include=['datetime64[ns]']).columns:\n",
    "        employee_df[col] = employee_df[col].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    \n",
    "    mongo_client = get_mongo_client(**mongodb_args)\n",
    "    db_name = mongodb_args['db_name']\n",
    "    collection_name = 'employees_from_mysql'\n",
    "    \n",
    "    db = mongo_client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    collection.drop()\n",
    "    \n",
    "    employee_df = employee_df.where(pd.notnull(employee_df), None)\n",
    "\n",
    "    \n",
    "    records = employee_df.to_dict('records')\n",
    "    \n",
    "    # Insert into MongoDB\n",
    "    collection.insert_many(records)\n",
    "    print(f\"Stored {len(records)} employee records in MongoDB collection: {collection_name}\")\n",
    "    mongo_client.close()\n",
    "    \n",
    "    return len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b0f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 5: Extract Data from MySQL\n",
    "# ------------------------------------------------------------------------------\n",
    "def extract_and_save_dimension_data():\n",
    "    print(\"Extracting dimension data from MySQL and MongoDB...\")\n",
    "    \n",
    "    # Extract Customer data\n",
    "    customer_query = \"\"\"\n",
    "    SELECT \n",
    "        c.CustomerID,\n",
    "        c.AccountNumber,\n",
    "        c.CustomerType,\n",
    "        a.AddressLine1,\n",
    "        a.AddressLine2,\n",
    "        a.City,\n",
    "        sp.Name AS StateProvinceName,\n",
    "        a.PostalCode,\n",
    "        cr.Name AS CountryRegionName\n",
    "    FROM customer c\n",
    "    LEFT JOIN customeraddress ca ON c.CustomerID = ca.CustomerID\n",
    "    LEFT JOIN address a ON ca.AddressID = a.AddressID\n",
    "    LEFT JOIN stateprovince sp ON a.StateProvinceID = sp.StateProvinceID\n",
    "    LEFT JOIN countryregion cr ON sp.CountryRegionCode = cr.CountryRegionCode\n",
    "    \"\"\"\n",
    "    customer_df = extract_from_mysql(customer_query)\n",
    "    print(f\"Extracted {len(customer_df)} customer records\")\n",
    "    customer_df.to_csv(os.path.join(batch_dir, 'dim_customer.csv'), index=False)\n",
    "\n",
    "    # Extract Product data\n",
    "    product_query = \"\"\"\n",
    "    SELECT\n",
    "        p.ProductID,\n",
    "        p.Name AS ProductName,\n",
    "        p.ProductNumber,\n",
    "        p.Color,\n",
    "        p.StandardCost,\n",
    "        p.ListPrice,\n",
    "        p.Size,\n",
    "        p.Weight,\n",
    "        p.ProductModelID,\n",
    "        pm.Name AS ProductModelName,\n",
    "        pc.Name AS ProductCategoryName,\n",
    "        psc.Name AS ProductSubcategoryName\n",
    "    FROM Product p\n",
    "    LEFT JOIN ProductModel pm ON p.ProductModelID = pm.ProductModelID\n",
    "    LEFT JOIN ProductSubcategory psc ON p.ProductSubcategoryID = psc.ProductSubcategoryID\n",
    "    LEFT JOIN ProductCategory pc ON psc.ProductCategoryID = pc.ProductCategoryID\n",
    "    WHERE p.FinishedGoodsFlag = 1\n",
    "    \"\"\"\n",
    "    product_df = extract_from_mysql(product_query)\n",
    "    print(f\"Extracted {len(product_df)} product records\")\n",
    "    product_df.to_csv(os.path.join(batch_dir, 'dim_product.csv'), index=False)\n",
    "    \n",
    "    mongo_client = get_mongo_client(**mongodb_args)\n",
    "    db_name = mongodb_args['db_name']\n",
    "    collection_name = 'employees_from_mysql'\n",
    "    \n",
    "    db = mongo_client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    employee_cursor = collection.find({})\n",
    "    employee_df = pd.DataFrame(list(employee_cursor))\n",
    "    \n",
    "    if '_id' in employee_df.columns:\n",
    "        employee_df = employee_df.drop('_id', axis=1)\n",
    "        \n",
    "    print(f\"Extracted {len(employee_df)} employee records from MongoDB\")\n",
    "    employee_df.to_csv(os.path.join(batch_dir, 'dim_employee.csv'), index=False)\n",
    "    mongo_client.close()\n",
    "    \n",
    "    dim_date_query = \"\"\"\n",
    "    SELECT \n",
    "        date_key,\n",
    "        full_date,\n",
    "        day_name_of_week,\n",
    "        month_name,\n",
    "        calendar_year,\n",
    "        calendar_quarter\n",
    "    FROM dim_date\n",
    "    \"\"\"\n",
    "    dim_date_df = extract_from_mysql(dim_date_query)\n",
    "    print(f\"Extracted {len(dim_date_df)} date dimension records\")\n",
    "    dim_date_df.to_csv(os.path.join(batch_dir, 'dim_date.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1de9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_lakehouse():\n",
    "    \"\"\"Setup the Spark session and database for the data lakehouse.\"\"\"\n",
    "    import os\n",
    "    import shutil  \n",
    "    from multiprocessing import cpu_count as get_cpu_count\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    cpu_count = get_cpu_count() or 2\n",
    "    jars = []\n",
    "    \n",
    "    mongodb_args = {\n",
    "        \"user_name\": \"mongodb_user\",\n",
    "        \"password\": \"mongodb_password\",\n",
    "        \"cluster_name\": \"cluster0\",\n",
    "        \"cluster_subnet\": \"abc123\",\n",
    "        \"db_name\": \"adventureworks\"\n",
    "    }\n",
    "\n",
    "    mysql_jar_path = []\n",
    "    mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "    mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "    mysql_jar_path.append(mysql_spark_jar)\n",
    "    #jars.append(mssql_spark_jar)\n",
    "\n",
    "    sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "    \n",
    "    # Create Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(sparkConf_args[\"app_name\"]) \\\n",
    "        .master(f\"local[{str(2 if cpu_count < 2 else cpu_count)}]\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", sparkConf_args[\"shuffle_partitions\"]) \\\n",
    "        .config(\"spark.jars\", sparkConf_args[\"spark_jars\"]) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    dest_database = \"adventure_works_dw\"\n",
    "    \n",
    "    print(f\"Checking if database {dest_database} exists...\")\n",
    "    \n",
    "    databases = [db.name for db in spark.catalog.listDatabases()]\n",
    "    \n",
    "    warehouse_dir = \"C:/Users/Baner/Downloads/spark-warehouse\"\n",
    "    db_location = os.path.join(warehouse_dir, f\"{dest_database}.db\")\n",
    "    \n",
    "    if dest_database in databases:\n",
    "        print(f\"Database {dest_database} already exists. Dropping it from Spark...\")\n",
    "        spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE\")\n",
    "    \n",
    "    if os.path.exists(db_location):\n",
    "        try:\n",
    "            print(f\"Removing directory: {db_location}\")\n",
    "            shutil.rmtree(db_location)\n",
    "            print(f\"Successfully removed {db_location}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not remove directory {db_location}: {str(e)}\")\n",
    "            print(\"You may need to manually delete this directory.\")\n",
    "    \n",
    "    sql_create_db = f\"\"\"\n",
    "        CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "        COMMENT 'DS-2002 Capstone'\n",
    "    \"\"\"\n",
    "    spark.sql(sql_create_db)\n",
    "    spark.sql(f\"USE {dest_database}\")\n",
    "    \n",
    "    get_file_info(batch_dir) \n",
    "    \n",
    "    # Populate the Customer Dimension\n",
    "    customer_csv = os.path.join(batch_dir, 'dim_customer.csv')\n",
    "    print(f\"Loading customer data from: {customer_csv}\")\n",
    "    \n",
    "    df_dim_customer = spark.read.format('csv').options(header='true', inferSchema='true').load(customer_csv)\n",
    "    print(f\"Loaded {df_dim_customer.count()} customer records\")\n",
    "    \n",
    "    df_dim_customer.createOrReplaceTempView(\"customers\")\n",
    "    sql_customers = \"\"\"\n",
    "        SELECT *, ROW_NUMBER() OVER (ORDER BY CustomerID) AS customer_key\n",
    "        FROM customers;\n",
    "    \"\"\"\n",
    "    df_dim_customer = spark.sql(sql_customers)\n",
    "    \n",
    "    df_dim_customer.write.saveAsTable(f\"{dest_database}.dim_customer\", mode=\"overwrite\")\n",
    "    print(f\"Saved customer data to {dest_database}.dim_customer\")\n",
    "\n",
    "    # Populate the Product Dimension\n",
    "    product_csv = os.path.join(batch_dir, 'dim_product.csv')\n",
    "    print(f\"Loading product data from: {product_csv}\")\n",
    "    \n",
    "    df_dim_product = spark.read.format('csv').options(header='true', inferSchema='true').load(product_csv)\n",
    "    print(f\"Loaded {df_dim_product.count()} product records\")\n",
    "    \n",
    "    df_dim_product.createOrReplaceTempView(\"products\")\n",
    "    sql_products = \"\"\"\n",
    "        SELECT *, ROW_NUMBER() OVER (ORDER BY ProductID) AS product_key\n",
    "        FROM products;\n",
    "    \"\"\"\n",
    "    df_dim_product = spark.sql(sql_products)\n",
    "    \n",
    "    df_dim_product.write.saveAsTable(f\"{dest_database}.dim_product\", mode=\"overwrite\")\n",
    "    print(f\"Saved product data to {dest_database}.dim_product\")\n",
    "    \n",
    "    # Populate the Employee Dimension\n",
    "    employee_csv = os.path.join(batch_dir, 'dim_employee.csv')\n",
    "    print(f\"Loading employee data from: {employee_csv}\")\n",
    "    \n",
    "    df_dim_employee = spark.read.format('csv').options(header='true', inferSchema='true').load(employee_csv)\n",
    "    print(f\"Loaded {df_dim_employee.count()} employee records\")\n",
    "    \n",
    "    df_dim_employee.createOrReplaceTempView(\"employees\")\n",
    "    sql_employees = \"\"\"\n",
    "        SELECT *, ROW_NUMBER() OVER (ORDER BY EmployeeID) AS employee_key\n",
    "        FROM employees;\n",
    "    \"\"\"\n",
    "    df_dim_employee = spark.sql(sql_employees)\n",
    "    \n",
    "    df_dim_employee.write.saveAsTable(f\"{dest_database}.dim_employee\", mode=\"overwrite\")\n",
    "    print(f\"Saved employee data to {dest_database}.dim_employee\")\n",
    "    \n",
    "    # Populate the Date Dimension\n",
    "    date_csv = os.path.join(batch_dir, 'dim_date.csv')\n",
    "    print(f\"Loading date data from: {date_csv}\")\n",
    "    \n",
    "    df_dim_date = spark.read.format('csv').options(header='true', inferSchema='true').load(date_csv)\n",
    "    print(f\"Loaded {df_dim_date.count()} date records\")\n",
    "    \n",
    "    df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")\n",
    "    print(f\"Saved date data to {dest_database}.dim_date\")\n",
    "    \n",
    "    # Populate the Territory Dimension \n",
    "    territory_csv = os.path.join(batch_dir, 'dim_territory.csv')\n",
    "    if os.path.exists(territory_csv):\n",
    "        print(f\"Loading territory data from: {territory_csv}\")\n",
    "        \n",
    "        df_dim_territory = spark.read.format('csv').options(header='true', inferSchema='true').load(territory_csv)\n",
    "        print(f\"Loaded {df_dim_territory.count()} territory records\")\n",
    "        \n",
    "        df_dim_territory.createOrReplaceTempView(\"territories\")\n",
    "        sql_territories = \"\"\"\n",
    "            SELECT *, ROW_NUMBER() OVER (ORDER BY TerritoryID) AS territory_key\n",
    "            FROM territories;\n",
    "        \"\"\"\n",
    "        df_dim_territory = spark.sql(sql_territories)\n",
    "        \n",
    "        df_dim_territory.write.saveAsTable(f\"{dest_database}.dim_territory\", mode=\"overwrite\")\n",
    "        print(f\"Saved territory data to {dest_database}.dim_territory\")\n",
    "    \n",
    "    print(\"\\nSample data from dimensions:\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nCustomer dimension:\")\n",
    "        spark.sql(f\"SELECT * FROM {dest_database}.dim_customer LIMIT 2\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying customer dimension: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nProduct dimension:\")\n",
    "        spark.sql(f\"SELECT * FROM {dest_database}.dim_product LIMIT 2\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying product dimension: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nEmployee dimension:\")\n",
    "        spark.sql(f\"SELECT * FROM {dest_database}.dim_employee LIMIT 2\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying employee dimension: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nDate dimension:\")\n",
    "        spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying date dimension: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nTerritory dimension:\")\n",
    "        spark.sql(f\"SELECT * FROM {dest_database}.dim_territory LIMIT 2\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying territory dimension: {str(e)}\")\n",
    "    \n",
    "    return spark, dest_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d37acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bronze_layer(spark, dest_database, bronze_dir, batch_dir, stream_dir):\n",
    "    import os\n",
    "    import time\n",
    "    import traceback\n",
    "    from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "    print(\"Loading Bronze layer...\")\n",
    "\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "\n",
    "    if not dest_database:\n",
    "        raise ValueError(\"Destination database name is required\")\n",
    "\n",
    "    bronze_paths = {\n",
    "        \"customers\": os.path.join(bronze_dir, \"dim_customer\"),\n",
    "        \"products\": os.path.join(bronze_dir, \"dim_product\"),\n",
    "        \"employees\": os.path.join(bronze_dir, \"dim_employee\"),\n",
    "        \"dates\": os.path.join(bronze_dir, \"dim_date\"),\n",
    "        \"territories\": os.path.join(bronze_dir, \"dim_territory\"),\n",
    "        \"sales\": os.path.join(bronze_dir, \"fact_sales\"),\n",
    "    }\n",
    "\n",
    "    for path in bronze_paths.values():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    print(f\"Creating database if not exists: {dest_database}\")\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {dest_database}\")\n",
    "\n",
    "    dim_tables = [\n",
    "        (\"customers\", 'dim_customer.csv'),\n",
    "        (\"products\", 'dim_product.csv'),\n",
    "        (\"employees\", 'dim_employee.csv'),\n",
    "        (\"dates\", 'dim_date.csv'),\n",
    "        (\"territories\", 'dim_territory.csv'),\n",
    "    ]\n",
    "\n",
    "    for name, filename in dim_tables:\n",
    "        source_path = os.path.join(batch_dir, filename)\n",
    "        dest_path = bronze_paths[name]\n",
    "\n",
    "        if os.path.exists(source_path):\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    df = spark.read.csv(source_path, header=True, inferSchema=True)\n",
    "                    df.write.mode(\"overwrite\").parquet(dest_path)\n",
    "                    print(f\"Loaded {df.count()} records to Bronze: {name}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {name}, attempt {attempt+1}/3: {e}\")\n",
    "                    time.sleep(2)\n",
    "            else:\n",
    "                print(f\"Failed to load {name} after 3 attempts\")\n",
    "        else:\n",
    "            print(f\"Warning: Source file not found: {source_path}\")\n",
    "\n",
    "    fact_schema = StructType([\n",
    "        StructField(\"SalesOrderID\", IntegerType(), True),\n",
    "        StructField(\"SalesOrderDetailID\", IntegerType(), True),\n",
    "        StructField(\"CustomerID\", IntegerType(), True),\n",
    "        StructField(\"ProductID\", IntegerType(), True),\n",
    "        StructField(\"EmployeeID\", IntegerType(), True),\n",
    "        StructField(\"TerritoryID\", IntegerType(), True),\n",
    "        StructField(\"OrderDate\", StringType(), True),\n",
    "        StructField(\"OrderQty\", IntegerType(), True),\n",
    "        StructField(\"UnitPrice\", FloatType(), True),\n",
    "        StructField(\"LineTotal\", FloatType(), True),\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        streaming_source = os.path.join(stream_dir, \"fact_sales\")\n",
    "        checkpoint = os.path.join(bronze_paths[\"sales\"], \"_checkpoint\")\n",
    "        os.makedirs(checkpoint, exist_ok=True)\n",
    "\n",
    "        stream_df = spark.readStream.schema(fact_schema).option(\"maxFilesPerTrigger\", 1).json(streaming_source)\n",
    "        query = stream_df.writeStream.format(\"parquet\").option(\"path\", bronze_paths[\"sales\"]).option(\"checkpointLocation\", checkpoint).trigger(once=True).start()\n",
    "        query.awaitTermination()\n",
    "        print(\"Completed streaming to Bronze layer\")\n",
    "    except Exception as e:\n",
    "        print(f\"Streaming error: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Register external tables\n",
    "    print(\"Registering bronze tables...\")\n",
    "    spark.sql(f\"USE {dest_database}\")\n",
    "\n",
    "    for name, path in bronze_paths.items():\n",
    "        table_name = f\"bronze_{name}\"\n",
    "\n",
    "        try:\n",
    "            files = [f for f in os.listdir(path) if f.endswith(\".parquet\") or f.endswith(\".snappy.parquet\")]\n",
    "            if not files:\n",
    "                print(f\"Skipping {table_name}: no Parquet files found in {path}\")\n",
    "                continue\n",
    "\n",
    "            df = spark.read.parquet(path)\n",
    "            df.limit(1).collect() \n",
    "\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {dest_database}.{table_name}\")\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE TABLE {dest_database}.{table_name}\n",
    "                USING PARQUET\n",
    "                LOCATION '{path.replace(\"\\\\\", \"/\")}'\n",
    "            \"\"\")\n",
    "            print(f\"Registered external table: {table_name}\")\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error registering table {table_name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(\"Bronze layer processing completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e114663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_silver_layer(spark, silver_dir, silver_database, bronze_database):\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    print(\"Step 5: Transforming and loading Silver layer...\")\n",
    "\n",
    "    if not silver_dir:\n",
    "        raise ValueError(\"silver_dir must be provided and cannot be empty\")\n",
    "\n",
    "    if not silver_database:\n",
    "        raise ValueError(\"silver_database must be provided and cannot be empty\")\n",
    "\n",
    "    if not bronze_database:\n",
    "        raise ValueError(\"bronze_database must be provided and cannot be empty\")\n",
    "\n",
    "    # Define Silver paths\n",
    "    silver_paths = {\n",
    "        \"customers\": os.path.join(silver_dir, \"dim_customer\"),\n",
    "        \"products\": os.path.join(silver_dir, \"dim_product\"),\n",
    "        \"employees\": os.path.join(silver_dir, \"dim_employee\"),\n",
    "        \"dates\": os.path.join(silver_dir, \"dim_date\"),\n",
    "        \"territories\": os.path.join(silver_dir, \"dim_territory\"),\n",
    "        \"sales\": os.path.join(silver_dir, \"fact_sales\"),\n",
    "    }\n",
    "    \n",
    "    for path in silver_paths.values():\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "    for path in silver_paths.values():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Create Silver database if not exists\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_database}\")\n",
    "    spark.sql(f\"USE {silver_database}\")\n",
    "\n",
    "    def write_silver_table(view_name, table_name, path):\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {silver_database}.{table_name}\")\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE {silver_database}.{table_name}\n",
    "            USING parquet\n",
    "            LOCATION '{path.replace(\"\\\\\", \"/\")}'\n",
    "            AS SELECT * FROM {view_name}\n",
    "        \"\"\")\n",
    "        print(f\" Created silver table: {silver_database}.{table_name}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Transform Customer Dimension\n",
    "    # ---------------------------\n",
    "    print(\"Transforming customer dimension...\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW silver_customers AS\n",
    "        SELECT\n",
    "            CustomerID,\n",
    "            AccountNumber,\n",
    "            CustomerType,\n",
    "            COALESCE(AddressLine1, 'Unknown') AS AddressLine1,\n",
    "            COALESCE(AddressLine2, '') AS AddressLine2,\n",
    "            COALESCE(City, 'Unknown') AS City,\n",
    "            COALESCE(StateProvinceName, 'Unknown') AS StateProvinceName,\n",
    "            COALESCE(PostalCode, 'Unknown') AS PostalCode,\n",
    "            COALESCE(CountryRegionName, 'Unknown') AS CountryRegionName,\n",
    "            current_timestamp() AS LastUpdated\n",
    "        FROM {bronze_database}.bronze_customers\n",
    "    \"\"\")\n",
    "    write_silver_table(\"silver_customers\", \"silver_customers\", silver_paths[\"customers\"])\n",
    "\n",
    "    # ---------------------------\n",
    "    # Transform Product Dimension\n",
    "    # ---------------------------\n",
    "    print(\"Transforming product dimension...\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW silver_products AS\n",
    "        SELECT\n",
    "            ProductID,\n",
    "            COALESCE(ProductName, 'Unknown') AS ProductName,\n",
    "            ProductNumber,\n",
    "            COALESCE(Color, 'N/A') AS Color,\n",
    "            StandardCost,\n",
    "            ListPrice,\n",
    "            COALESCE(Size, 'N/A') AS Size,\n",
    "            Weight,\n",
    "            ProductModelID,\n",
    "            COALESCE(ProductModelName, 'Unknown') AS ProductModelName,\n",
    "            COALESCE(ProductCategoryName, 'Unknown') AS ProductCategoryName,\n",
    "            COALESCE(ProductSubcategoryName, 'Unknown') AS ProductSubcategoryName,\n",
    "            current_timestamp() AS LastUpdated\n",
    "        FROM {bronze_database}.bronze_products\n",
    "    \"\"\")\n",
    "    write_silver_table(\"silver_products\", \"silver_products\", silver_paths[\"products\"])\n",
    "\n",
    "    # ---------------------------\n",
    "    # Transform Employee Dimension\n",
    "    # ---------------------------\n",
    "    print(\"Transforming employee dimension...\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW silver_employees AS\n",
    "        SELECT\n",
    "            EmployeeID,\n",
    "            NationalIDNumber,\n",
    "            LoginID,\n",
    "            ManagerID,\n",
    "            COALESCE(Title, 'Unknown') AS Title,\n",
    "            BirthDate,\n",
    "            MaritalStatus,\n",
    "            Gender,\n",
    "            HireDate,\n",
    "            SalariedFlag,\n",
    "            VacationHours,\n",
    "            SickLeaveHours,\n",
    "            DepartmentID,\n",
    "            ShiftID,\n",
    "            StartDate,\n",
    "            EndDate,\n",
    "            current_timestamp() AS LastUpdated\n",
    "        FROM {bronze_database}.bronze_employees\n",
    "    \"\"\")\n",
    "    write_silver_table(\"silver_employees\", \"silver_employees\", silver_paths[\"employees\"])\n",
    "\n",
    "    # ---------------------------\n",
    "    # Transform Date Dimension\n",
    "    # ---------------------------\n",
    "    print(\"Transforming date dimension...\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW silver_dates AS\n",
    "        SELECT\n",
    "            date_key,\n",
    "            full_date,\n",
    "            day_name_of_week,\n",
    "            month_name,\n",
    "            calendar_year,\n",
    "            calendar_quarter,\n",
    "            current_timestamp() AS LastUpdated\n",
    "        FROM {bronze_database}.bronze_dates\n",
    "    \"\"\")\n",
    "    write_silver_table(\"silver_dates\", \"silver_dates\", silver_paths[\"dates\"])\n",
    "\n",
    "    # ---------------------------\n",
    "    # Transform Territory Dimension\n",
    "    # ---------------------------\n",
    "    print(\"Transforming territory dimension...\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW silver_territories AS\n",
    "        SELECT\n",
    "            TerritoryID,\n",
    "            COALESCE(TerritoryName, 'Unknown') AS TerritoryName,\n",
    "            COALESCE(CountryRegionCode, 'Unknown') AS CountryRegionCode,\n",
    "            COALESCE(TerritoryGroup, 'Unknown') AS TerritoryGroup,\n",
    "            current_timestamp() AS LastUpdated\n",
    "        FROM {bronze_database}.bronze_territories\n",
    "    \"\"\")\n",
    "    write_silver_table(\"silver_territories\", \"silver_territories\", silver_paths[\"territories\"])\n",
    "\n",
    "    # ---------------------------\n",
    "    # Transform Sales Fact Table\n",
    "    # ---------------------------\n",
    "    print(\"Transforming sales fact data...\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW silver_sales AS\n",
    "        SELECT\n",
    "            SalesOrderID,\n",
    "            SalesOrderDetailID,\n",
    "            CustomerID,\n",
    "            ProductID,\n",
    "            COALESCE(EmployeeID, -1) AS EmployeeID,\n",
    "            COALESCE(TerritoryID, -1) AS TerritoryID,\n",
    "            to_date(OrderDate) AS OrderDate,\n",
    "            year(to_date(OrderDate)) AS OrderYear,\n",
    "            month(to_date(OrderDate)) AS OrderMonth,\n",
    "            dayofmonth(to_date(OrderDate)) AS OrderDay,\n",
    "            quarter(to_date(OrderDate)) AS OrderQuarter,\n",
    "            OrderQty,\n",
    "            UnitPrice,\n",
    "            LineTotal,\n",
    "            current_timestamp() AS LastUpdated\n",
    "        FROM {bronze_database}.bronze_sales\n",
    "    \"\"\")\n",
    "    write_silver_table(\"silver_sales\", \"silver_sales\", silver_paths[\"sales\"])\n",
    "\n",
    "    print(\"Silver layer transformation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afe8e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_layer(spark, gold_dir, dest_database):\n",
    "    import os\n",
    "\n",
    "    print(\"Loading Gold layer...\")\n",
    "\n",
    "    if not gold_dir:\n",
    "        raise ValueError(\"gold_dir must be provided and cannot be empty\")\n",
    "\n",
    "    if not dest_database:\n",
    "        raise ValueError(\"dest_database must be provided and cannot be empty\")\n",
    "\n",
    "    # Define Gold paths\n",
    "    gold_paths = {\n",
    "        \"sales_by_product\": os.path.join(gold_dir, \"sales_by_product\"),\n",
    "        \"sales_by_territory\": os.path.join(gold_dir, \"sales_by_territory\"),\n",
    "        \"sales_by_customer\": os.path.join(gold_dir, \"sales_by_customer\"),\n",
    "        \"sales_by_date\": os.path.join(gold_dir, \"sales_by_date\"),\n",
    "        \"top_products\": os.path.join(gold_dir, \"top_products\"),\n",
    "    }\n",
    "\n",
    "    # Create directories\n",
    "    for path in gold_paths.values():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Use the destination database\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {dest_database}\")\n",
    "    spark.sql(f\"USE {dest_database}\")\n",
    "\n",
    "    def write_gold_table(view_name, table_name, path):\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {dest_database}.{table_name}\")\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE {dest_database}.{table_name}\n",
    "            USING parquet\n",
    "            LOCATION '{path.replace(\"\\\\\", \"/\")}'\n",
    "            AS SELECT * FROM {view_name}\n",
    "        \"\"\")\n",
    "        print(f\" Created gold table: {dest_database}.{table_name}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Sales by Product\n",
    "    # ------------------------------\n",
    "    print(\"Creating sales by product analytics view...\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW gold_sales_by_product AS\n",
    "        SELECT\n",
    "            p.ProductID,\n",
    "            p.ProductName,\n",
    "            p.ProductCategoryName,\n",
    "            p.ProductSubcategoryName,\n",
    "            SUM(s.OrderQty) AS TotalQuantity,\n",
    "            SUM(s.LineTotal) AS TotalSales,\n",
    "            COUNT(DISTINCT s.SalesOrderID) AS NumberOfOrders,\n",
    "            AVG(s.UnitPrice) AS AverageUnitPrice\n",
    "        FROM silver_sales s\n",
    "        JOIN silver_products p ON s.ProductID = p.ProductID\n",
    "        GROUP BY p.ProductID, p.ProductName, p.ProductCategoryName, p.ProductSubcategoryName\n",
    "    \"\"\")\n",
    "    write_gold_table(\"gold_sales_by_product\", \"gold_sales_by_product\", gold_paths[\"sales_by_product\"])\n",
    "\n",
    "    # ------------------------------\n",
    "    # Sales by Territory\n",
    "    # ------------------------------\n",
    "    print(\"Creating sales by territory analytics view...\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW gold_sales_by_territory AS\n",
    "        SELECT\n",
    "            t.TerritoryID,\n",
    "            t.TerritoryName,\n",
    "            t.CountryRegionCode,\n",
    "            t.TerritoryGroup,\n",
    "            SUM(s.LineTotal) AS TotalSales,\n",
    "            COUNT(DISTINCT s.SalesOrderID) AS NumberOfOrders,\n",
    "            COUNT(DISTINCT s.CustomerID) AS NumberOfCustomers,\n",
    "            AVG(s.LineTotal) AS AverageOrderValue\n",
    "        FROM silver_sales s\n",
    "        JOIN silver_territories t ON s.TerritoryID = t.TerritoryID\n",
    "        GROUP BY t.TerritoryID, t.TerritoryName, t.CountryRegionCode, t.TerritoryGroup\n",
    "    \"\"\")\n",
    "    write_gold_table(\"gold_sales_by_territory\", \"gold_sales_by_territory\", gold_paths[\"sales_by_territory\"])\n",
    "\n",
    "    # ------------------------------\n",
    "    # Sales by Customer\n",
    "    # ------------------------------\n",
    "    print(\"Creating sales by customer analytics view...\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW gold_sales_by_customer AS\n",
    "        SELECT\n",
    "            c.CustomerID,\n",
    "            c.AccountNumber,\n",
    "            c.CustomerType,\n",
    "            c.CountryRegionName,\n",
    "            c.StateProvinceName,\n",
    "            c.City,\n",
    "            SUM(s.LineTotal) AS TotalSpend,\n",
    "            COUNT(DISTINCT s.SalesOrderID) AS NumberOfOrders,\n",
    "            COUNT(DISTINCT s.ProductID) AS NumberOfUniqueProducts,\n",
    "            MIN(s.OrderDate) AS FirstPurchaseDate,\n",
    "            MAX(s.OrderDate) AS LastPurchaseDate,\n",
    "            AVG(s.LineTotal) AS AverageOrderValue\n",
    "        FROM silver_sales s\n",
    "        JOIN silver_customers c ON s.CustomerID = c.CustomerID\n",
    "        GROUP BY c.CustomerID, c.AccountNumber, c.CustomerType, c.CountryRegionName, c.StateProvinceName, c.City\n",
    "    \"\"\")\n",
    "    write_gold_table(\"gold_sales_by_customer\", \"gold_sales_by_customer\", gold_paths[\"sales_by_customer\"])\n",
    "\n",
    "    # ------------------------------\n",
    "    # Sales by Date\n",
    "    # ------------------------------\n",
    "    print(\"Creating sales by date analytics view...\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW gold_sales_by_date AS\n",
    "        SELECT\n",
    "            s.OrderDate,\n",
    "            s.OrderYear,\n",
    "            s.OrderMonth,\n",
    "            s.OrderQuarter,\n",
    "            SUM(s.LineTotal) AS TotalSales,\n",
    "            COUNT(DISTINCT s.SalesOrderID) AS NumberOfOrders,\n",
    "            COUNT(DISTINCT s.CustomerID) AS NumberOfCustomers,\n",
    "            AVG(s.LineTotal) AS AverageOrderValue,\n",
    "            COUNT(DISTINCT s.ProductID) AS NumberOfUniqueProducts\n",
    "        FROM silver_sales s\n",
    "        GROUP BY s.OrderDate, s.OrderYear, s.OrderMonth, s.OrderQuarter\n",
    "    \"\"\")\n",
    "    write_gold_table(\"gold_sales_by_date\", \"gold_sales_by_date\", gold_paths[\"sales_by_date\"])\n",
    "\n",
    "    # ------------------------------\n",
    "    # Top Products\n",
    "    # ------------------------------\n",
    "    print(\"Creating top products analytics view...\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW gold_top_products AS\n",
    "        SELECT\n",
    "            p.ProductID,\n",
    "            p.ProductName,\n",
    "            p.ProductCategoryName,\n",
    "            p.ProductSubcategoryName,\n",
    "            SUM(s.LineTotal) AS TotalSales,\n",
    "            SUM(s.OrderQty) AS TotalQuantity,\n",
    "            ROW_NUMBER() OVER (ORDER BY SUM(s.LineTotal) DESC) AS SalesRank,\n",
    "            ROW_NUMBER() OVER (ORDER BY SUM(s.OrderQty) DESC) AS QuantityRank\n",
    "        FROM silver_sales s\n",
    "        JOIN silver_products p ON s.ProductID = p.ProductID\n",
    "        GROUP BY p.ProductID, p.ProductName, p.ProductCategoryName, p.ProductSubcategoryName\n",
    "    \"\"\")\n",
    "    write_gold_table(\"gold_top_products\", \"gold_top_products\", gold_paths[\"top_products\"])\n",
    "\n",
    "    print(\" Gold layer analytics tables created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52841c56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AdventureWorks Data Lakehouse Implementation\n",
      "================================================================================\n",
      "\n",
      "Step 1: Setting up directory structure...\n",
      "Extracting employee data from MySQL and uploading to MongoDB...\n",
      "Extracted 296 employee records with department and shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baner\\AppData\\Local\\Temp\\ipykernel_18796\\705343402.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 296 employee records in MongoDB collection: employees_from_mysql\n",
      "\n",
      "Step 2: Extracting data from source systems...\n",
      "Extracting dimension data from MySQL and MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baner\\AppData\\Local\\Temp\\ipykernel_18796\\705343402.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 19220 customer records\n",
      "Extracted 295 product records\n",
      "Extracted 296 employee records from MongoDB\n",
      "Extracted 4018 date dimension records\n",
      "\n",
      "Step 3: Setting up the Data Lakehouse...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baner\\AppData\\Local\\Temp\\ipykernel_18796\\705343402.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if database adventure_works_dw exists...\n",
      "Removing directory: C:/Users/Baner/Downloads/spark-warehouse\\adventure_works_dw.db\n",
      "Successfully removed C:/Users/Baner/Downloads/spark-warehouse\\adventure_works_dw.db\n",
      "Loading customer data from: C:\\Users\\Baner\\Downloads\\lab_data\\adventureworks\\batch\\dim_customer.csv\n",
      "Loaded 19220 customer records\n",
      "Saved customer data to adventure_works_dw.dim_customer\n",
      "Loading product data from: C:\\Users\\Baner\\Downloads\\lab_data\\adventureworks\\batch\\dim_product.csv\n",
      "Loaded 295 product records\n",
      "Saved product data to adventure_works_dw.dim_product\n",
      "Loading employee data from: C:\\Users\\Baner\\Downloads\\lab_data\\adventureworks\\batch\\dim_employee.csv\n",
      "Loaded 296 employee records\n",
      "Saved employee data to adventure_works_dw.dim_employee\n",
      "Loading date data from: C:\\Users\\Baner\\Downloads\\lab_data\\adventureworks\\batch\\dim_date.csv\n",
      "Loaded 4018 date records\n",
      "Saved date data to adventure_works_dw.dim_date\n",
      "Loading territory data from: C:\\Users\\Baner\\Downloads\\lab_data\\adventureworks\\batch\\dim_territory.csv\n",
      "Loaded 10 territory records\n",
      "Saved territory data to adventure_works_dw.dim_territory\n",
      "\n",
      "Sample data from dimensions:\n",
      "\n",
      "Customer dimension:\n",
      "+----------+-------------+------------+------------------+------------+-------+-----------------+----------+-----------------+------------+\n",
      "|CustomerID|AccountNumber|CustomerType|      AddressLine1|AddressLine2|   City|StateProvinceName|PostalCode|CountryRegionName|customer_key|\n",
      "+----------+-------------+------------+------------------+------------+-------+-----------------+----------+-----------------+------------+\n",
      "|         1|   AW00000001|           S|2251 Elliot Avenue|        NULL|Seattle|       Washington|     98104|    United States|           1|\n",
      "|         2|   AW00000002|           S|   7943 Walnut Ave|        NULL| Renton|       Washington|     98055|    United States|           2|\n",
      "+----------+-------------+------------+------------------+------------+-------+-----------------+----------+-----------------+------------+\n",
      "\n",
      "\n",
      "Product dimension:\n",
      "+---------+--------------------+-------------+-----+------------+---------+----+------+--------------+----------------+-------------------+----------------------+-----------+\n",
      "|ProductID|         ProductName|ProductNumber|Color|StandardCost|ListPrice|Size|Weight|ProductModelID|ProductModelName|ProductCategoryName|ProductSubcategoryName|product_key|\n",
      "+---------+--------------------+-------------+-----+------------+---------+----+------+--------------+----------------+-------------------+----------------------+-----------+\n",
      "|      680|HL Road Frame - B...|   FR-R92B-58|Black|     1059.31|   1431.5|  58|  2.24|             6|   HL Road Frame|         Components|           Road Frames|          1|\n",
      "|      706|HL Road Frame - R...|   FR-R92R-58|  Red|     1059.31|   1431.5|  58|  2.24|             6|   HL Road Frame|         Components|           Road Frames|          2|\n",
      "+---------+--------------------+-------------+-----+------------+---------+----+------+--------------+----------------+-------------------+----------------------+-----------+\n",
      "\n",
      "\n",
      "Employee dimension:\n",
      "+----------+----------------+--------------------+---------+--------------------+----------+-------------+------+----------+------------+-------------+--------------+------------+--------------+-------+---------+----------+-------+------------+\n",
      "|EmployeeID|NationalIDNumber|             LoginID|ManagerID|               Title| BirthDate|MaritalStatus|Gender|  HireDate|SalariedFlag|VacationHours|SickLeaveHours|DepartmentID|DepartmentName|ShiftID|ShiftName| StartDate|EndDate|employee_key|\n",
      "+----------+----------------+--------------------+---------+--------------------+----------+-------------+------+----------+------------+-------------+--------------+------------+--------------+-------+---------+----------+-------+------------+\n",
      "|         1|        14417807|adventure-works\\guy1|     16.0|Production Techni...|1972-05-15|            M|     M|1996-07-31|     b'\\x00'|           21|            30|           7|    Production|      1|      Day|1996-07-31|   NULL|           1|\n",
      "|         2|       253022876|adventure-works\\k...|      6.0| Marketing Assistant|1977-06-03|            S|     M|1997-02-26|     b'\\x00'|           42|            41|           4|     Marketing|      1|      Day|1997-02-26|   NULL|           2|\n",
      "+----------+----------------+--------------------+---------+--------------------+----------+-------------+------+----------+------------+-------------+--------------+------------+--------------+-------+---------+----------+-------+------------+\n",
      "\n",
      "\n",
      "Date dimension:\n",
      "+--------+----------+----------------+----------+-------------+----------------+\n",
      "|date_key| full_date|day_name_of_week|month_name|calendar_year|calendar_quarter|\n",
      "+--------+----------+----------------+----------+-------------+----------------+\n",
      "|20000101|2000-01-01|        Saturday|   January|         2000|               1|\n",
      "|20000102|2000-01-02|          Sunday|   January|         2000|               1|\n",
      "+--------+----------+----------------+----------+-------------+----------------+\n",
      "\n",
      "\n",
      "Territory dimension:\n",
      "+-----------+-------------+-----------------+--------------+-------------+\n",
      "|TerritoryID|TerritoryName|CountryRegionCode|TerritoryGroup|territory_key|\n",
      "+-----------+-------------+-----------------+--------------+-------------+\n",
      "|          1|    Northwest|               US| North America|            1|\n",
      "|          2|    Northeast|               US| North America|            2|\n",
      "+-----------+-------------+-----------------+--------------+-------------+\n",
      "\n",
      "\n",
      "Step 3: filling up the Data Lakehouse...\n",
      "\n",
      "Step 4: Loading Bronze layer...\n",
      "Loading Bronze layer...\n",
      "Creating database if not exists: adventureworks_bronze\n",
      "Loaded 19220 records to Bronze: customers\n",
      "Loaded 295 records to Bronze: products\n",
      "Loaded 296 records to Bronze: employees\n",
      "Loaded 4018 records to Bronze: dates\n",
      "Loaded 10 records to Bronze: territories\n",
      "Completed streaming to Bronze layer\n",
      "Registering bronze tables...\n",
      "Registered external table: bronze_customers\n",
      "Registered external table: bronze_products\n",
      "Registered external table: bronze_employees\n",
      "Registered external table: bronze_dates\n",
      "Registered external table: bronze_territories\n",
      "Registered external table: bronze_sales\n",
      " Bronze layer processing completed.\n",
      "\n",
      "Step 5: Transforming and loading Silver layer...\n",
      "Step 5: Transforming and loading Silver layer...\n",
      "Transforming customer dimension...\n",
      " Created silver table: adventureworks_silver.silver_customers\n",
      "Transforming product dimension...\n",
      " Created silver table: adventureworks_silver.silver_products\n",
      "Transforming employee dimension...\n",
      " Created silver table: adventureworks_silver.silver_employees\n",
      "Transforming date dimension...\n",
      " Created silver table: adventureworks_silver.silver_dates\n",
      "Transforming territory dimension...\n",
      " Created silver table: adventureworks_silver.silver_territories\n",
      "Transforming sales fact data...\n",
      " Created silver table: adventureworks_silver.silver_sales\n",
      " Silver layer transformation complete.\n",
      "\n",
      "Step 6: Creating Gold layer analytics views...\n",
      "Loading Gold layer...\n",
      "Creating sales by product analytics view...\n",
      " Created gold table: adventure_works_dw.gold_sales_by_product\n",
      "Creating sales by territory analytics view...\n",
      " Created gold table: adventure_works_dw.gold_sales_by_territory\n",
      "Creating sales by customer analytics view...\n",
      " Created gold table: adventure_works_dw.gold_sales_by_customer\n",
      "Creating sales by date analytics view...\n",
      " Created gold table: adventure_works_dw.gold_sales_by_date\n",
      "Creating top products analytics view...\n",
      " Created gold table: adventure_works_dw.gold_top_products\n",
      " Gold layer analytics tables created successfully.\n",
      "\n",
      "Step 7: Running sample analytics queries...\n",
      "\n",
      "Running sample analytics queries on Gold layer...\n",
      "\n",
      "Top 5 products by total sales:\n",
      "               ProductName ProductCategoryName ProductSubcategoryName  \\\n",
      "0   Mountain-200 Black, 38               Bikes         Mountain Bikes   \n",
      "1   Mountain-200 Black, 42               Bikes         Mountain Bikes   \n",
      "2  Mountain-200 Silver, 38               Bikes         Mountain Bikes   \n",
      "3  Mountain-200 Silver, 42               Bikes         Mountain Bikes   \n",
      "4  Mountain-200 Silver, 46               Bikes         Mountain Bikes   \n",
      "\n",
      "     TotalSales  SalesRank  \n",
      "0  4.400593e+06          1  \n",
      "1  4.009495e+06          2  \n",
      "2  3.693678e+06          3  \n",
      "3  3.438479e+06          4  \n",
      "4  3.434257e+06          5  \n",
      "\n",
      "\n",
      "Sales by territory group:\n",
      "  TerritoryGroup    GroupSales  GroupOrders  GroupAvgOrderValue\n",
      "0  North America  7.935336e+07        16108         1103.088511\n",
      "1         Europe  1.983768e+07         8514          728.868823\n",
      "2        Pacific  1.065534e+07         6843          707.619606\n",
      "\n",
      "\n",
      "Monthly sales trend:\n",
      "   OrderYear  OrderMonth  MonthlySales  MonthlyOrders\n",
      "0        NaN         NaN  1.098464e+08          31465\n",
      "\n",
      "\n",
      "Customer type analysis:\n",
      "  CustomerType  CustomerCount    TotalSpend  AvgCustomerSpend  \\\n",
      "0            S            635  8.270253e+07     130240.197326   \n",
      "1            I          18484  2.937924e+07       1588.496159   \n",
      "\n",
      "   AvgOrdersPerCustomer  \n",
      "0              5.993701  \n",
      "1              1.496188  \n",
      "\n",
      "\n",
      "Product category performance:\n",
      "  ProductCategoryName  CategorySales  CategoryQuantity  NumberOfProducts  \\\n",
      "0               Bikes   9.465117e+07             90268                97   \n",
      "1          Components   1.180259e+07             49044               111   \n",
      "2            Clothing   2.120543e+06             73670                34   \n",
      "3         Accessories   1.272073e+06             61932                24   \n",
      "\n",
      "   AveragePriceInCategory  \n",
      "0             1165.319194  \n",
      "1              248.555519  \n",
      "2               36.116773  \n",
      "3               28.501913  \n",
      "\n",
      "\n",
      "\n",
      "Data Lakehouse implementation completed successfully!\n",
      "\n",
      "Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 10: Query the Gold Layer\n",
    "# ------------------------------------------------------------------------------\n",
    "def query_gold_layer(spark, dest_database):\n",
    "    print(\"\\nRunning sample analytics queries on Gold layer...\\n\")\n",
    "    \n",
    "    # Use the database\n",
    "    spark.sql(f\"USE {dest_database}\")\n",
    "    \n",
    "    # Sample query 1: Top 5 products by total sales\n",
    "    print(\"Top 5 products by total sales:\")\n",
    "    result = spark.sql(\"\"\"\n",
    "    SELECT ProductName, ProductCategoryName, ProductSubcategoryName, \n",
    "           TotalSales, SalesRank\n",
    "    FROM gold_top_products\n",
    "    WHERE SalesRank <= 5\n",
    "    ORDER BY SalesRank\n",
    "    \"\"\").toPandas()\n",
    "    print(result)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Sample query 2: Sales by territory group\n",
    "    print(\"Sales by territory group:\")\n",
    "    result = spark.sql(\"\"\"\n",
    "    SELECT TerritoryGroup, \n",
    "           SUM(TotalSales) AS GroupSales, \n",
    "           SUM(NumberOfOrders) AS GroupOrders,\n",
    "           AVG(AverageOrderValue) AS GroupAvgOrderValue\n",
    "    FROM gold_sales_by_territory\n",
    "    GROUP BY TerritoryGroup\n",
    "    ORDER BY GroupSales DESC\n",
    "    \"\"\").toPandas()\n",
    "    print(result)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Sample query 3: Monthly sales trend\n",
    "    print(\"Monthly sales trend:\")\n",
    "    result = spark.sql(\"\"\"\n",
    "    SELECT OrderYear, OrderMonth, \n",
    "           SUM(TotalSales) AS MonthlySales,\n",
    "           SUM(NumberOfOrders) AS MonthlyOrders\n",
    "    FROM gold_sales_by_date\n",
    "    GROUP BY OrderYear, OrderMonth\n",
    "    ORDER BY OrderYear, OrderMonth\n",
    "    \"\"\").toPandas()\n",
    "    print(result)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Sample query 4: Top customer types by spending\n",
    "    print(\"Customer type analysis:\")\n",
    "    result = spark.sql(\"\"\"\n",
    "    SELECT CustomerType, \n",
    "           COUNT(DISTINCT CustomerID) AS CustomerCount,\n",
    "           SUM(TotalSpend) AS TotalSpend,\n",
    "           AVG(TotalSpend) AS AvgCustomerSpend,\n",
    "           AVG(NumberOfOrders) AS AvgOrdersPerCustomer\n",
    "    FROM gold_sales_by_customer\n",
    "    GROUP BY CustomerType\n",
    "    ORDER BY TotalSpend DESC\n",
    "    \"\"\").toPandas()\n",
    "    print(result)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Sample query 5: Product category performance\n",
    "    print(\"Product category performance:\")\n",
    "    result = spark.sql(\"\"\"\n",
    "    SELECT ProductCategoryName, \n",
    "           SUM(TotalSales) AS CategorySales,\n",
    "           SUM(TotalQuantity) AS CategoryQuantity,\n",
    "           COUNT(DISTINCT ProductID) AS NumberOfProducts,\n",
    "           AVG(AverageUnitPrice) AS AveragePriceInCategory\n",
    "    FROM gold_sales_by_product\n",
    "    GROUP BY ProductCategoryName\n",
    "    ORDER BY CategorySales DESC\n",
    "    \"\"\").toPandas()\n",
    "    print(result)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Main Execution\n",
    "# ------------------------------------------------------------------------------\n",
    "def main():\n",
    "    base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "    data_dir = os.path.join(base_dir, 'adventureworks')\n",
    "    batch_dir = os.path.join(data_dir, 'batch')\n",
    "    stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "    # Set up the output directories for our lakehouse\n",
    "    lakehouse_dir = os.path.join(base_dir, 'adventureworks_lakehouse')\n",
    "    os.makedirs(lakehouse_dir, exist_ok=True)\n",
    "\n",
    "    # Set up paths for bronze, silver, gold layers\n",
    "    bronze_dir = os.path.join(lakehouse_dir, 'bronze')\n",
    "    silver_dir = os.path.join(lakehouse_dir, 'silver')\n",
    "    gold_dir = os.path.join(lakehouse_dir, 'gold')\n",
    "    print(\"=\" * 80)\n",
    "    print(\"AdventureWorks Data Lakehouse Implementation\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Create directory structure\n",
    "    print(\"\\nStep 1: Setting up directory structure...\")\n",
    "    os.makedirs(batch_dir, exist_ok=True)\n",
    "    os.makedirs(stream_dir, exist_ok=True)\n",
    "    \n",
    "    upload_employee_data_to_mongodb()\n",
    "    \n",
    "    # Step 2: Extract data from MySQL and store in CSV and JSON files\n",
    "    print(\"\\nStep 2: Extracting data from source systems...\")\n",
    "    extract_and_save_dimension_data()\n",
    "    \n",
    "    # Step 3: Setup the Data Lakehouse\n",
    "    print(\"\\nStep 3: Setting up the Data Lakehouse...\")\n",
    "    spark, dest_database = setup_lakehouse()\n",
    "    \n",
    "    #Step 4: Populate Warehouse\n",
    "    print(\"\\nStep 3: filling up the Data Lakehouse...\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Step 4: Load data into Bronze layer\n",
    "        print(\"\\nStep 4: Loading Bronze layer...\")\n",
    "        load_bronze_layer(spark, \"adventureworks_bronze\", bronze_dir, batch_dir, stream_dir)\n",
    "\n",
    "        # Step 5: Transform data into Silver layer\n",
    "        print(\"\\nStep 5: Transforming and loading Silver layer...\")\n",
    "        load_silver_layer(spark, silver_dir, silver_database=\"adventureworks_silver\", bronze_database=\"adventureworks_bronze\")\n",
    "\n",
    "        \n",
    "        # Step 6: Create Gold layer analytics views\n",
    "        print(\"\\nStep 6: Creating Gold layer analytics views...\")\n",
    "        load_gold_layer(spark, gold_dir, dest_database)\n",
    "        \n",
    "        # Step 7: Run sample queries on Gold layer\n",
    "        print(\"\\nStep 7: Running sample analytics queries...\")\n",
    "        query_gold_layer(spark, dest_database)\n",
    "        \n",
    "        print(\"\\nData Lakehouse implementation completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "        print(\"\\nSpark session stopped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0af153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pysparkenv)",
   "language": "python",
   "name": "pysparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
